<!DOCTYPE html>
<html>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../static/scripts/indexScript.js"></script>
        <script src="../static/scripts/guide4Script.js"></script>
        <script src="../static/scripts/notify.js"></script>
        <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
        <link rel="stylesheet" href="../static/stylesheets/generalStyle.css">
        <link rel="stylesheet" href="../static/stylesheets/progress.css">
        <link rel="stylesheet" href="../static/stylesheets/guideStyle.css">
        <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
        <title>NNViz</title>
    </head>
    <body>
        <div id="navbarDiv">
            <nav class="navbar navbar-expand-lg" id="navbar">
                <div class="logo">
                    <img src="../static/images/NNViz%20Logo%202.png" width="200" alt="" id="logo">
                </div>
                <div class="navbar-collapse collapse w-100 order-1 order-md-0 dual-collapse2" id="navbarSupportedContent">
                    <ul class="navbar-nav me-auto">
                        <li class="nav-item">
                            <a class="nav-link" id="homeLink" href="/">Home</a>
                        </li>
                    </ul>
                </div>
            </nav>
        </div>
        <div class="header">
            <h2><b>Step 4:</b> Configure Loss Function</h2>
            <div class="progress">
                <div class="circle done">
                    <a href="/guide/1"><span class="label">1</span></a>
                    <p id="input" class="title">Input</p>
                </div>
                <span class="bar done"></span>
                <div class="circle done">
                    <a href="/guide/2"><span class="label">2</span></a>
                    <p id="architecture" class="title">Architecture</p>
                </div>
                <span class="bar done"></span>
                <div class="circle done">
                    <a href="/guide/3"><span class="label">3</span></a>
                    <p id="activation" class="title">Activation</p>
                </div>
{#                <span class="bar"></span>#}
{#                <div class="circle">#}
{#                    <span class="label">4</span>#}
{#                    <p id="regularizer" class="title">&nbsp;&nbsp;Initialization &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and  Regularization</p>#}
{#                </div>#}
                <span class="bar done"></span>
                <div class="circle active">
                    <a href="/guide/4"><span class="label">4</span></a>
                    <p id="loss" class="title">&nbsp;&nbsp;&nbsp;Loss &nbsp;Function</p>
                </div>
                <span class="bar active"></span>
                <div class="circle">
                    <a href="/guide/5"><span class="label">5</span></a>
                    <p id="optimizer" class="title">Optimizer</p>
                </div>
            </div>
            <h2>
                <a href="/guide/3"><button class="btn-primary btn">Previous Step</button></a>
                <a href="/guide/5"><button class="btn-primary btn">Next Step</button></a>
            </h2>
        </div>
        <div id="content">
            <div class="introduction" id="why-div">
                <ul>
                    <li>
                        <p>
                            As we mentioned in the previous step of the guide, the learning process of a neural network consists of repeatedly adjusting
                            the network's parameters (weights and biases) in order to minimize the difference between the predicted label (output) of the
                            network and the actual label from the training dataset. That difference represents the <b>error</b> of the neural network, and
                            it is computed with the help of a <b>loss function</b>.

                            In other words, the adjusting of the network parameters is done with the intent to <b>minimize the cost function</b>. Therefore, a
                            loss function's role is to evaluate the network's error.
                        </p>
                    </li>
                    <li>
                        <h4>Mean Squared Error</h4>
                        <p>
                            <b>Mean Squared Error</b> (or <b>MSE</b>) is computed by averaging the squared differences between the predicted labels and the actual
                            labels. If we had \(n\) training instances and labels \(t\), for which the network predicted \(n\) outputs \(y\), then the formula for the
                            <b>MSE</b> is:
                            \[
                                L_{MSE} = \frac{1}{2n} \sum_{i}^{n}(t_i - y_i)^{2}
                            \]
                            where \(t_i\) is the actual label provided in the training dataset instance \(i\) and \(y_i\) is the predicted label by the network for the same instance.
                        </p>
                        <p>
                            One thing to note about the MSE is that its formula resembles the one for the variance, which computes how far some elements are
                            from the mean.
                        </p>
                        <p>
                            The Mean Squared Error is continuous and easily differentiable, which will be very useful in the training process, since the <b>Backpropagation</b>
                            algorithm uses the gradient of the loss function extensively in the training process:
                            \[
                                L_{MSE}' = -\frac{1}{n} \sum_{i}^{n}(t_i - y_i)y_i'
                            \]
                            where \(y_i'\) is the <b>gradient of the activation function</b>.
                        </p>
                    </li>
                    <br>
                    <li>
                        <h4>Cross-Entropy</h4>
                        <p>
                            The <b>Cross-Entropy</b> loss function is best used together with the <b>Softmax</b> activation function, because its purpose is to take output probabilities and
                            measure the "distance" from the actual values (the labels provided in the training dataset). Therefore, each predicted class probability is compared to the
                            actual class label (which is processed beforehand to its probability distribution form) and a <b>loss value</b> (i.e. a penalty) is computed. This value is
                            logarithmic, yielding a large values (close to \(1\)) for large differences and a small values (close to \(0\)) for small differences.
                        </p>
                        <p>
                            The Cross-Entropy function, for \(n\) training instances across \(m\) different classes is defined as
                            \[
                                L_{CE} = - \frac{1}{n} \sum_i^n ( \sum_j^m t_j \log(y_j) )
                            \]
                            where \(t_i\) is the actual truth value (either \(0\) or \(1\)) of the an instance \(i\) from the class \(j\) (a value provided in the training dataset) and \(y_j\)
                            is the predicted probability for the same instance.
                        </p>
                        <p>
                            In conclusion, this loss function is best suited for <b>multi-class classification problems</b> and is best used together with the <b>Softmax</b> activation function.
                        </p>
                    </li>
                    <br>
                    <li>
                        <h4>Binary Cross-Entropy</h4>
                        <p>
                            The <b>Binary Cross-Entropy</b> loss function is a particular version of the Cross-Entropy, used in binary classification problems.
                        </p>
                        <p>
                            Formula, for \(n\) training instances:
                            \[
                                L_{BCE} = - \frac{1}{n} \sum_i^n(t_i \log(y_i) + (1 - t_i) \log(1-y_i))
                            \]
                        </p>
                        <p> As a result, this loss function is best suited for <b>binary classification problems</b> and is best used together with the <b>Sigmoid</b> activation on the output layer</p>
                    </li>
                </ul>
                <p>
                    Having all of these in mind, you may choose a loss function for your configured network. You can also go back to the <a href="/guide/3">previous step</a> and change activation function on different layers.
                </p>
            </div>
            <br>
            <div class="configure-nn">
                <form id="configure-loss-form">
                    <div class="row">
                        <div class="col">
                            <div class="form-group">
                                <label for="loss-function" id="loss-label" class="form-label"></label>
                                <select id="loss-function" class="form-select">
                                    <option selected value="mean_squared_error">Mean Squared Error</option>
                                    <option value="categorical_crossentropy">Cross-Entropy</option>
                                    <option value="binary_crossentropy">Binary Cross-Entropy</option>
                                </select>
                            </div>
                            <div class="col">
                                <button id="confirm-network-loss" class="btn btn-primary">Confirm network loss function</button>
                            </div>
                        </div>
                    </div>

                </form>
            </div>
        </div>
    </body>
</html>